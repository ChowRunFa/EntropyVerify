nohup: ignoring input
INFO 11-21 17:11:40 [utils.py:253] non-default args: {'disable_log_stats': True, 'model': '/cephfs/qiuwentao/models/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218'}
INFO 11-21 17:11:41 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 11-21 17:11:41 [model.py:1745] Using max model len 40960
INFO 11-21 17:12:14 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 11-21 17:12:16 [system_utils.py:103] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized
[1;36m(EngineCore_DP0 pid=16664)[0;0m INFO 11-21 17:26:57 [core.py:93] Initializing a V1 LLM engine (v0.11.1) with config: model='/cephfs/qiuwentao/models/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218', speculative_config=None, tokenizer='/cephfs/qiuwentao/models/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/cephfs/qiuwentao/models/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=16664)[0;0m INFO 11-21 17:29:20 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.233.98.146:54311 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=16664)[0;0m INFO 11-21 17:29:23 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=16664)[0;0m INFO 11-21 17:29:25 [gpu_model_runner.py:3255] Starting to load model /cephfs/qiuwentao/models/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218...
Traceback (most recent call last):
  File "/cephfs/shared/conda/envs/pea/lib/python3.11/site-packages/tvm_ffi/utils/_build_optional_torch_c_dlpack.py", line 836, in <module>
    main()
  File "/cephfs/shared/conda/envs/pea/lib/python3.11/site-packages/tvm_ffi/utils/_build_optional_torch_c_dlpack.py", line 762, in main
    include_paths.extend(get_torch_include_paths(args.build_with_cuda))
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cephfs/shared/conda/envs/pea/lib/python3.11/site-packages/tvm_ffi/utils/_build_optional_torch_c_dlpack.py", line 676, in get_torch_include_paths
    return torch.utils.cpp_extension.include_paths(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cephfs/shared/conda/envs/pea/lib/python3.11/site-packages/torch/utils/cpp_extension.py", line 1518, in include_paths
    cuda_home_include = _join_cuda_home('include')
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cephfs/shared/conda/envs/pea/lib/python3.11/site-packages/torch/utils/cpp_extension.py", line 2986, in _join_cuda_home
    raise OSError('CUDA_HOME environment variable is not set. '
OSError: CUDA_HOME environment variable is not set. Please set it to your CUDA install root.
[1;36m(EngineCore_DP0 pid=16664)[0;0m /cephfs/shared/conda/envs/pea/lib/python3.11/site-packages/tvm_ffi/_optional_torch_c_dlpack.py:106: UserWarning: Failed to JIT torch c dlpack extension, EnvTensorAllocator will not be enabled.
[1;36m(EngineCore_DP0 pid=16664)[0;0m You may try AOT-module via `pip install torch-c-dlpack-ext`
[1;36m(EngineCore_DP0 pid=16664)[0;0m   warnings.warn(
[1;36m(EngineCore_DP0 pid=16664)[0;0m INFO 11-21 17:33:31 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=16664)[0;0m INFO 11-21 17:33:31 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=16664)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=16664)[0;0m Loading safetensors checkpoint shards:  20% Completed | 1/5 [02:25<09:41, 145.28s/it]
[1;36m(EngineCore_DP0 pid=16664)[0;0m Loading safetensors checkpoint shards:  40% Completed | 2/5 [05:08<07:47, 155.86s/it]
[1;36m(EngineCore_DP0 pid=16664)[0;0m Loading safetensors checkpoint shards:  60% Completed | 3/5 [07:38<05:06, 153.07s/it]
[1;36m(EngineCore_DP0 pid=16664)[0;0m Loading safetensors checkpoint shards:  80% Completed | 4/5 [08:26<01:51, 111.81s/it]
[1;36m(EngineCore_DP0 pid=16664)[0;0m Loading safetensors checkpoint shards: 100% Completed | 5/5 [10:40<00:00, 119.84s/it]
[1;36m(EngineCore_DP0 pid=16664)[0;0m Loading safetensors checkpoint shards: 100% Completed | 5/5 [10:41<00:00, 128.30s/it]
[1;36m(EngineCore_DP0 pid=16664)[0;0m 
[1;36m(EngineCore_DP0 pid=16664)[0;0m INFO 11-21 17:44:17 [default_loader.py:314] Loading weights took 641.64 seconds
[1;36m(EngineCore_DP0 pid=16664)[0;0m INFO 11-21 17:44:31 [gpu_model_runner.py:3334] Model loading took 15.2683 GiB memory and 892.865622 seconds
[1;36m(EngineCore_DP0 pid=16664)[0;0m INFO 11-21 17:49:57 [backends.py:631] Using cache directory: /root/.cache/vllm/torch_compile_cache/7c001245d9/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=16664)[0;0m INFO 11-21 17:49:57 [backends.py:647] Dynamo bytecode transform time: 325.09 s
[1;36m(EngineCore_DP0 pid=16664)[0;0m INFO 11-21 17:50:28 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 29.576 s
[1;36m(EngineCore_DP0 pid=16664)[0;0m INFO 11-21 17:50:50 [monitor.py:34] torch.compile takes 354.67 s in total
[1;36m(EngineCore_DP0 pid=16664)[0;0m INFO 11-21 17:51:07 [gpu_worker.py:359] Available KV cache memory: 7.65 GiB
[1;36m(EngineCore_DP0 pid=16664)[0;0m INFO 11-21 17:51:08 [kv_cache_utils.py:1229] GPU KV cache size: 55,712 tokens
[1;36m(EngineCore_DP0 pid=16664)[0;0m INFO 11-21 17:51:08 [kv_cache_utils.py:1234] Maximum concurrency for 40,960 tokens per request: 1.36x
[1;36m(EngineCore_DP0 pid=16664)[0;0m ERROR 11-21 17:51:10 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=16664)[0;0m ERROR 11-21 17:51:10 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=16664)[0;0m ERROR 11-21 17:51:10 [core.py:842]   File "/cephfs/shared/conda/envs/pea/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=16664)[0;0m ERROR 11-21 17:51:10 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=16664)[0;0m ERROR 11-21 17:51:10 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=16664)[0;0m ERROR 11-21 17:51:10 [core.py:842]   File "/cephfs/shared/conda/envs/pea/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=16664)[0;0m ERROR 11-21 17:51:10 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=16664)[0;0m ERROR 11-21 17:51:10 [core.py:842]   File "/cephfs/shared/conda/envs/pea/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=16664)[0;0m ERROR 11-21 17:51:10 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=16664)[0;0m ERROR 11-21 17:51:10 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=16664)[0;0m ERROR 11-21 17:51:10 [core.py:842]   File "/cephfs/shared/conda/envs/pea/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=16664)[0;0m ERROR 11-21 17:51:10 [core.py:842]     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=16664)[0;0m ERROR 11-21 17:51:10 [core.py:842]   File "/cephfs/shared/conda/envs/pea/lib/python3.11/site-packages/vllm/v1/executor/abstract.py", line 115, in initialize_from_config
[1;36m(EngineCore_DP0 pid=16664)[0;0m ERROR 11-21 17:51:10 [core.py:842]     self.collective_rpc("initialize_from_config", args=(kv_cache_configs,))
[1;36m(EngineCore_DP0 pid=16664)[0;0m ERROR 11-21 17:51:10 [core.py:842]   File "/cephfs/shared/conda/envs/pea/lib/python3.11/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=16664)[0;0m ERROR 11-21 17:51:10 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=16664)[0;0m ERROR 11-21 17:51:10 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=16664)[0;0m ERROR 11-21 17:51:10 [core.py:842]   File "/cephfs/shared/conda/envs/pea/lib/python3.11/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=16664)[0;0m ERROR 11-21 17:51:10 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=16664)[0;0m ERROR 11-21 17:51:10 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=16664)[0;0m ERROR 11-21 17:51:10 [core.py:842]   File "/cephfs/shared/conda/envs/pea/lib/python3.11/site-packages/vllm/v1/worker/worker_base.py", line 319, in initialize_from_config
[1;36m(EngineCore_DP0 pid=16664)[0;0m ERROR 11-21 17:51:10 [core.py:842]     self.worker.initialize_from_config(kv_cache_config)  # type: ignore
[1;36m(EngineCore_DP0 pid=16664)[0;0m ERROR 11-21 17:51:10 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=16664)[0;0m ERROR 11-21 17:51:10 [core.py:842]   File "/cephfs/shared/conda/envs/pea/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py", line 404, in initialize_from_config
[1;36m(EngineCore_DP0 pid=16664)[0;0m ERROR 11-21 17:51:10 [core.py:842]     self.model_runner.initialize_kv_cache(kv_cache_config)
[1;36m(EngineCore_DP0 pid=16664)[0;0m ERROR 11-21 17:51:10 [core.py:842]   File "/cephfs/shared/conda/envs/pea/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py", line 5003, in initialize_kv_cache
[1;36m(EngineCore_DP0 pid=16664)[0;0m ERROR 11-21 17:51:10 [core.py:842]     kv_caches = self.initialize_kv_cache_tensors(
[1;36m(EngineCore_DP0 pid=16664)[0;0m ERROR 11-21 17:51:10 [core.py:842]                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=16664)[0;0m ERROR 11-21 17:51:10 [core.py:842]   File "/cephfs/shared/conda/envs/pea/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4929, in initialize_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=16664)[0;0m ERROR 11-21 17:51:10 [core.py:842]     kv_cache_raw_tensors = self._allocate_kv_cache_tensors(kv_cache_config)
[1;36m(EngineCore_DP0 pid=16664)[0;0m ERROR 11-21 17:51:10 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=16664)[0;0m ERROR 11-21 17:51:10 [core.py:842]   File "/cephfs/shared/conda/envs/pea/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4714, in _allocate_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=16664)[0;0m ERROR 11-21 17:51:10 [core.py:842]     tensor = torch.zeros(
[1;36m(EngineCore_DP0 pid=16664)[0;0m ERROR 11-21 17:51:10 [core.py:842]              ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=16664)[0;0m ERROR 11-21 17:51:10 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 218.00 MiB. GPU 0 has a total capacity of 44.55 GiB of which 45.75 MiB is free. Process 12181 has 21.22 GiB memory in use. Process 24343 has 23.28 GiB memory in use. Of the allocated memory 20.88 GiB is allocated by PyTorch, and 10.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=16664)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=16664)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=16664)[0;0m   File "/cephfs/shared/conda/envs/pea/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=16664)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=16664)[0;0m   File "/cephfs/shared/conda/envs/pea/lib/python3.11/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=16664)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=16664)[0;0m   File "/cephfs/shared/conda/envs/pea/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=16664)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=16664)[0;0m   File "/cephfs/shared/conda/envs/pea/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=16664)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=16664)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=16664)[0;0m   File "/cephfs/shared/conda/envs/pea/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=16664)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=16664)[0;0m   File "/cephfs/shared/conda/envs/pea/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=16664)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=16664)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=16664)[0;0m   File "/cephfs/shared/conda/envs/pea/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=16664)[0;0m     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=16664)[0;0m   File "/cephfs/shared/conda/envs/pea/lib/python3.11/site-packages/vllm/v1/executor/abstract.py", line 115, in initialize_from_config
[1;36m(EngineCore_DP0 pid=16664)[0;0m     self.collective_rpc("initialize_from_config", args=(kv_cache_configs,))
[1;36m(EngineCore_DP0 pid=16664)[0;0m   File "/cephfs/shared/conda/envs/pea/lib/python3.11/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=16664)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=16664)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=16664)[0;0m   File "/cephfs/shared/conda/envs/pea/lib/python3.11/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=16664)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=16664)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=16664)[0;0m   File "/cephfs/shared/conda/envs/pea/lib/python3.11/site-packages/vllm/v1/worker/worker_base.py", line 319, in initialize_from_config
[1;36m(EngineCore_DP0 pid=16664)[0;0m     self.worker.initialize_from_config(kv_cache_config)  # type: ignore
[1;36m(EngineCore_DP0 pid=16664)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=16664)[0;0m   File "/cephfs/shared/conda/envs/pea/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py", line 404, in initialize_from_config
[1;36m(EngineCore_DP0 pid=16664)[0;0m     self.model_runner.initialize_kv_cache(kv_cache_config)
[1;36m(EngineCore_DP0 pid=16664)[0;0m   File "/cephfs/shared/conda/envs/pea/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py", line 5003, in initialize_kv_cache
[1;36m(EngineCore_DP0 pid=16664)[0;0m     kv_caches = self.initialize_kv_cache_tensors(
[1;36m(EngineCore_DP0 pid=16664)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=16664)[0;0m   File "/cephfs/shared/conda/envs/pea/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4929, in initialize_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=16664)[0;0m     kv_cache_raw_tensors = self._allocate_kv_cache_tensors(kv_cache_config)
[1;36m(EngineCore_DP0 pid=16664)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=16664)[0;0m   File "/cephfs/shared/conda/envs/pea/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4714, in _allocate_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=16664)[0;0m     tensor = torch.zeros(
[1;36m(EngineCore_DP0 pid=16664)[0;0m              ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=16664)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 218.00 MiB. GPU 0 has a total capacity of 44.55 GiB of which 45.75 MiB is free. Process 12181 has 21.22 GiB memory in use. Process 24343 has 23.28 GiB memory in use. Of the allocated memory 20.88 GiB is allocated by PyTorch, and 10.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W1121 17:51:15.959339591 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/root/projects/PEA/get_entropy.py", line 199, in <module>
    model = LLM(MODEL_PATH)
            ^^^^^^^^^^^^^^^
  File "/cephfs/shared/conda/envs/pea/lib/python3.11/site-packages/vllm/entrypoints/llm.py", line 343, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cephfs/shared/conda/envs/pea/lib/python3.11/site-packages/vllm/v1/engine/llm_engine.py", line 174, in from_engine_args
    return cls(
           ^^^^
  File "/cephfs/shared/conda/envs/pea/lib/python3.11/site-packages/vllm/v1/engine/llm_engine.py", line 108, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cephfs/shared/conda/envs/pea/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cephfs/shared/conda/envs/pea/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 640, in __init__
    super().__init__(
  File "/cephfs/shared/conda/envs/pea/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 469, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
  File "/cephfs/shared/conda/envs/pea/lib/python3.11/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/cephfs/shared/conda/envs/pea/lib/python3.11/site-packages/vllm/v1/engine/utils.py", line 907, in launch_core_engines
    wait_for_engine_startup(
  File "/cephfs/shared/conda/envs/pea/lib/python3.11/site-packages/vllm/v1/engine/utils.py", line 964, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
